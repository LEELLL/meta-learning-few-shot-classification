{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "resnet18_miniImagenet_train.ipynb",
      "provenance": [],
      "collapsed_sections": [],
      "toc_visible": true,
      "machine_shape": "hm",
      "include_colab_link": true
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "accelerator": "GPU",
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "4cc592b3c15e4cb484765df2cf128ae6": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HBoxModel",
          "state": {
            "_view_name": "HBoxView",
            "_dom_classes": [],
            "_model_name": "HBoxModel",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "box_style": "",
            "layout": "IPY_MODEL_051c30c901684682a672c8fe19b94bd9",
            "_model_module": "@jupyter-widgets/controls",
            "children": [
              "IPY_MODEL_f8d42ae62ea646de82236db6e41a3543",
              "IPY_MODEL_8065d0adbb684d8b8fe48a9c0aefb473"
            ]
          }
        },
        "051c30c901684682a672c8fe19b94bd9": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "f8d42ae62ea646de82236db6e41a3543": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "IntProgressModel",
          "state": {
            "_view_name": "ProgressView",
            "style": "IPY_MODEL_0c53a06680e84674a7364c0d625a5bae",
            "_dom_classes": [],
            "description": "100%",
            "_model_name": "IntProgressModel",
            "bar_style": "success",
            "max": 46827520,
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": 46827520,
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "orientation": "horizontal",
            "min": 0,
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_f80268db6fea41bd99c7bd838fbb408f"
          }
        },
        "8065d0adbb684d8b8fe48a9c0aefb473": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "HTMLModel",
          "state": {
            "_view_name": "HTMLView",
            "style": "IPY_MODEL_b8ed4a8a00544ddeb01aabe585b93a93",
            "_dom_classes": [],
            "description": "",
            "_model_name": "HTMLModel",
            "placeholder": "â€‹",
            "_view_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "value": " 44.7M/44.7M [00:33&lt;00:00, 1.41MB/s]",
            "_view_count": null,
            "_view_module_version": "1.5.0",
            "description_tooltip": null,
            "_model_module": "@jupyter-widgets/controls",
            "layout": "IPY_MODEL_9d96b8059ad94cc1b66fa677144af952"
          }
        },
        "0c53a06680e84674a7364c0d625a5bae": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "ProgressStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "ProgressStyleModel",
            "description_width": "initial",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "bar_color": null,
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "f80268db6fea41bd99c7bd838fbb408f": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        },
        "b8ed4a8a00544ddeb01aabe585b93a93": {
          "model_module": "@jupyter-widgets/controls",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_view_name": "StyleView",
            "_model_name": "DescriptionStyleModel",
            "description_width": "",
            "_view_module": "@jupyter-widgets/base",
            "_model_module_version": "1.5.0",
            "_view_count": null,
            "_view_module_version": "1.2.0",
            "_model_module": "@jupyter-widgets/controls"
          }
        },
        "9d96b8059ad94cc1b66fa677144af952": {
          "model_module": "@jupyter-widgets/base",
          "model_name": "LayoutModel",
          "state": {
            "_view_name": "LayoutView",
            "grid_template_rows": null,
            "right": null,
            "justify_content": null,
            "_view_module": "@jupyter-widgets/base",
            "overflow": null,
            "_model_module_version": "1.2.0",
            "_view_count": null,
            "flex_flow": null,
            "width": null,
            "min_width": null,
            "border": null,
            "align_items": null,
            "bottom": null,
            "_model_module": "@jupyter-widgets/base",
            "top": null,
            "grid_column": null,
            "overflow_y": null,
            "overflow_x": null,
            "grid_auto_flow": null,
            "grid_area": null,
            "grid_template_columns": null,
            "flex": null,
            "_model_name": "LayoutModel",
            "justify_items": null,
            "grid_row": null,
            "max_height": null,
            "align_content": null,
            "visibility": null,
            "align_self": null,
            "height": null,
            "min_height": null,
            "padding": null,
            "grid_auto_rows": null,
            "grid_gap": null,
            "max_width": null,
            "order": null,
            "_view_module_version": "1.2.0",
            "grid_template_areas": null,
            "object_position": null,
            "object_fit": null,
            "grid_auto_columns": null,
            "margin": null,
            "display": null,
            "left": null
          }
        }
      }
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/sirandou/meta-learning-few-shot-classification/blob/master/resnet18_miniImagenet_train.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xmx3jbv530kk",
        "colab_type": "text"
      },
      "source": [
        "# Setting up Google Colab"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "JHl-HKk_3Mug",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "!pip install -U -q PyDrive"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "zrmPXVo13jhE",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "from pydrive.auth import GoogleAuth\n",
        "from pydrive.drive import GoogleDrive\n",
        "from google.colab import auth\n",
        "from oauth2client.client import GoogleCredentials"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "rhsb8Cnf3lSg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "auth.authenticate_user()\n",
        "gauth = GoogleAuth()\n",
        "gauth.credentials = GoogleCredentials.get_application_default()\n",
        "drive = GoogleDrive(gauth)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "llgTurVolX7t",
        "colab_type": "code",
        "outputId": "c4789903-823c-4e8c-bbf8-285cabe68865",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 122
        }
      },
      "source": [
        "\n",
        "from google.colab import drive\n",
        "drive.mount('/content/drive')"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Go to this URL in a browser: https://accounts.google.com/o/oauth2/auth?client_id=947318989803-6bn6qk8qdgf4n4g3pfee6491hc0brc4i.apps.googleusercontent.com&redirect_uri=urn%3aietf%3awg%3aoauth%3a2.0%3aoob&response_type=code&scope=email%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdocs.test%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive%20https%3a%2f%2fwww.googleapis.com%2fauth%2fdrive.photos.readonly%20https%3a%2f%2fwww.googleapis.com%2fauth%2fpeopleapi.readonly\n",
            "\n",
            "Enter your authorization code:\n",
            "Â·Â·Â·Â·Â·Â·Â·Â·Â·Â·\n",
            "Mounted at /content/drive\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UtErM4-bcDde",
        "colab_type": "code",
        "outputId": "1d4963bd-1b15-48fb-b630-1cd115b9dfc4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "cd 'drive/My Drive/ML project'"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/content/drive/My Drive/ML project\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "I_x4IPkk_ZWc",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision\n",
        "import torchvision.datasets as dset\n",
        "import torchvision.transforms as transforms\n",
        "import torch\n",
        "from torch.utils.data import DataLoader,Dataset\n",
        "import random\n",
        "import os\n",
        "from PIL import Image\n",
        "import matplotlib.pyplot as plt\n",
        "import numpy as np\n",
        "from torch.utils.data.sampler import Sampler"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "N_sjGFRyTfrH",
        "colab_type": "text"
      },
      "source": [
        "#Getting data folders"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Kz0rJ5ITqWrg",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mini_imagenet_folders():\n",
        "    train_folder = 'datas/miniImagenet/train'\n",
        "    test_folder = 'datas/miniImagenet/test'                \n",
        "    val_folder = 'datas/miniImagenet/val'\n",
        "\n",
        "    metatrain_folders = [os.path.join(train_folder, label) \\\n",
        "                for label in os.listdir(train_folder) \\\n",
        "                if os.path.isdir(os.path.join(train_folder, label)) \\\n",
        "                ]\n",
        "    metatest_folders = [os.path.join(test_folder, label) \\\n",
        "                for label in os.listdir(test_folder) \\\n",
        "                if os.path.isdir(os.path.join(test_folder, label)) \\\n",
        "                ]\n",
        "    metaval_folders = [os.path.join(val_folder, label) \\\n",
        "                for label in os.listdir(val_folder) \\\n",
        "                if os.path.isdir(os.path.join(val_folder, label)) \\\n",
        "                ]\n",
        "\n",
        "    random.seed(1)\n",
        "    random.shuffle(metatrain_folders)\n",
        "    random.shuffle(metatest_folders)\n",
        "    random.shuffle(metaval_folders)\n",
        "\n",
        "    return metatrain_folders,metatest_folders, metaval_folders"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "QK7M2Orp_xog",
        "colab_type": "code",
        "outputId": "32b9a096-b8bf-43bb-b9f8-fea7339cf574",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 34
        }
      },
      "source": [
        "# Step 1: init data folders\n",
        "print(\"init data folders\")\n",
        "# init character folders for dataset construction\n",
        "metatrain_folders,metatest_folders,metaval_folders = mini_imagenet_folders()"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "init data folders\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "trI_zmS7Tpfn",
        "colab_type": "text"
      },
      "source": [
        "#Hyperparameters"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "oJeVI7wWAmSN",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "from torch.autograd import Variable\n",
        "from torch.optim.lr_scheduler import StepLR\n",
        "import numpy as np\n",
        "import os\n",
        "import math\n",
        "import argparse\n",
        "import scipy as sp\n",
        "import scipy.stats\n",
        "\n",
        "\n",
        "# Hyper Parameters\n",
        "FEATURE_DIM = 32\n",
        "RELATION_DIM = 8\n",
        "CLASS_NUM = 5\n",
        "SAMPLE_NUM_PER_CLASS = 5   #5\n",
        "BATCH_NUM_PER_CLASS = 10   #10\n",
        "EPISODE = 1000\n",
        "TEST_EPISODE = 20\n",
        "LEARNING_RATE = 0.001\n",
        "GPU = 0\n",
        "HIDDEN_UNIT = 10"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "a8TP0r34aGCi",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def weights_init(m):\n",
        "    classname = m.__class__.__name__\n",
        "    if classname.find('Conv') != -1:\n",
        "        n = m.kernel_size[0] * m.kernel_size[1] * m.out_channels\n",
        "        m.weight.data.normal_(0, math.sqrt(2. / n))\n",
        "        if m.bias is not None:\n",
        "            m.bias.data.zero_()\n",
        "    elif classname.find('BatchNorm') != -1:\n",
        "        m.weight.data.fill_(1)\n",
        "        m.bias.data.zero_()\n",
        "    elif classname.find('Linear') != -1:\n",
        "        n = m.weight.size(1)\n",
        "        m.weight.data.normal_(0, 0.01)\n",
        "        m.bias.data = torch.ones(m.bias.data.size())"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "RhEQlGhyEOzA",
        "colab_type": "code",
        "outputId": "4a8f32c6-9a82-477d-e688-b9ab36f29da4",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 51
        }
      },
      "source": [
        "#if os.path.exists(str(\"miniimagenet/models/miniimagenet_feature_encoder_\" + str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\")):\n",
        "#    feature_encoder.load_state_dict(torch.load(str(\"miniimagenet/models/miniimagenet_feature_encoder_\" + str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"),map_location='cuda:0'))\n",
        "#    print(\"load feature encoder success\")\n",
        "#if os.path.exists(str(\"miniimagenet/models/miniimagenet_relation_network_\"+ str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\")):\n",
        "#    relation_network.load_state_dict(torch.load(str(\"miniimagenet/models/miniimagenet_relation_network_\"+ str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"),map_location='cuda:0'))\n",
        "#    print(\"load relation network success\")"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "load feature encoder success\n",
            "load relation network success\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "V6Y85slNVbDe",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "def mean_confidence_interval(data, confidence=0.95):\n",
        "    a = 1.0*np.array(data)\n",
        "    n = len(a)\n",
        "    m, se = np.mean(a), scipy.stats.sem(a)\n",
        "    h = se * sp.stats.t._ppf((1+confidence)/2., n-1)\n",
        "    return m,h"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NfXY9SgQT4Yb",
        "colab_type": "text"
      },
      "source": [
        "#Loading datasets"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Jw9uNk3cFhrq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class Rotate(object):\n",
        "    def __init__(self, angle):\n",
        "        self.angle = angle\n",
        "    def __call__(self, x, mode=\"reflect\"):\n",
        "        x = x.rotate(self.angle)\n",
        "        return x\n",
        "\n",
        "class MiniImagenetTask(object):\n",
        "\n",
        "    def __init__(self, character_folders, num_classes, train_num,test_num):\n",
        "\n",
        "        self.character_folders = character_folders\n",
        "        self.num_classes = num_classes\n",
        "        self.train_num = train_num\n",
        "        self.test_num = test_num\n",
        "\n",
        "        class_folders = random.sample(self.character_folders,self.num_classes)\n",
        "        labels = np.array(range(len(class_folders)))\n",
        "        labels = dict(zip(class_folders, labels))\n",
        "        samples = dict()\n",
        "\n",
        "        self.train_roots = []\n",
        "        self.test_roots = []\n",
        "        for c in class_folders:\n",
        "\n",
        "            temp = [os.path.join(c, x) for x in os.listdir(c)]\n",
        "            samples[c] = random.sample(temp, len(temp))\n",
        "            random.shuffle(samples[c])\n",
        "\n",
        "            self.train_roots += samples[c][:train_num]\n",
        "            self.test_roots += samples[c][train_num:train_num+test_num]\n",
        "\n",
        "        self.train_labels = [labels[self.get_class(x)] for x in self.train_roots]\n",
        "        self.test_labels = [labels[self.get_class(x)] for x in self.test_roots]\n",
        "\n",
        "    def get_class(self, sample):\n",
        "        return os.path.join(*sample.split('/')[:-1])\n",
        "\n",
        "class FewShotDataset(Dataset):\n",
        "\n",
        "    def __init__(self, task, split='train', transform=None, target_transform=None):\n",
        "        self.transform = transform # Torch operations on the input image\n",
        "        self.target_transform = target_transform\n",
        "        self.task = task\n",
        "        self.split = split\n",
        "        self.image_roots = self.task.train_roots if self.split == 'train' else self.task.test_roots\n",
        "        self.labels = self.task.train_labels if self.split == 'train' else self.task.test_labels\n",
        "\n",
        "    def __len__(self):\n",
        "        return len(self.image_roots)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        raise NotImplementedError(\"This is an abstract class. Subclass this class for your particular dataset.\")\n",
        "\n",
        "class MiniImagenet(FewShotDataset):\n",
        "\n",
        "    def __init__(self, *args, **kwargs):\n",
        "        super(MiniImagenet, self).__init__(*args, **kwargs)\n",
        "\n",
        "    def __getitem__(self, idx):\n",
        "        image_root = self.image_roots[idx]\n",
        "        image = Image.open(image_root)\n",
        "        image = image.convert('RGB')\n",
        "        if self.transform is not None:\n",
        "            image = self.transform(image)\n",
        "        label = self.labels[idx]\n",
        "        if self.target_transform is not None:\n",
        "            label = self.target_transform(label)\n",
        "        return image, label\n",
        "\n",
        "\n",
        "class ClassBalancedSampler(Sampler):\n",
        "    ''' Samples 'num_inst' examples each from 'num_cl' pools\n",
        "        of examples of size 'num_per_class' '''\n",
        "\n",
        "    def __init__(self, num_cl, num_inst,shuffle=True):\n",
        "\n",
        "        self.num_cl = num_cl\n",
        "        self.num_inst = num_inst\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        # return a single list of indices, assuming that items will be grouped by class\n",
        "        if self.shuffle:\n",
        "            batches = [[i+j*self.num_inst for i in torch.randperm(self.num_inst)] for j in range(self.num_cl)]\n",
        "        else:\n",
        "            batches = [[i+j*self.num_inst for i in range(self.num_inst)] for j in range(self.num_cl)]\n",
        "        batches = [[batches[j][i] for j in range(self.num_cl)] for i in range(self.num_inst)]\n",
        "\n",
        "        if self.shuffle:\n",
        "            random.shuffle(batches)\n",
        "            for sublist in batches:\n",
        "                   random.shuffle(sublist)\n",
        "        batches = [item for sublist in batches for item in sublist]\n",
        "        return iter(batches)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "class ClassBalancedSamplerOld(Sampler):\n",
        "    ''' Samples 'num_inst' examples each from 'num_cl' pools\n",
        "        of examples of size 'num_per_class' '''\n",
        "\n",
        "    def __init__(self, num_per_class, num_cl, num_inst,shuffle=True):\n",
        "        self.num_per_class = num_per_class\n",
        "        self.num_cl = num_cl\n",
        "        self.num_inst = num_inst\n",
        "        self.shuffle = shuffle\n",
        "\n",
        "    def __iter__(self):\n",
        "        # return a single list of indices, assuming that items will be grouped by class\n",
        "        if self.shuffle:\n",
        "            batch = [[i+j*self.num_inst for i in torch.randperm(self.num_inst)[:self.num_per_class]] for j in range(self.num_cl)]\n",
        "        else:\n",
        "            batch = [[i+j*self.num_inst for i in range(self.num_inst)[:self.num_per_class]] for j in range(self.num_cl)]\n",
        "        batch = [item for sublist in batch for item in sublist]\n",
        "\n",
        "        if self.shuffle:\n",
        "            random.shuffle(batch)\n",
        "        return iter(batch)\n",
        "\n",
        "    def __len__(self):\n",
        "        return 1\n",
        "\n",
        "\n",
        "def get_mini_imagenet_data_loader(task, num_per_class=1, split='train',shuffle = False):\n",
        "    #normalize = transforms.Normalize(mean=[0.92206, 0.92206, 0.92206], std=[0.08426, 0.08426, 0.08426])\n",
        "    normalize = transforms.Normalize(mean=[0.485, 0.456, 0.406],std=[0.229, 0.224, 0.225])\n",
        "    resizing =  transforms.Resize(224, interpolation=2)\n",
        "    dataset = MiniImagenet(task,split=split,transform=transforms.Compose([resizing,transforms.ToTensor(),normalize]))\n",
        "    if split == 'train':\n",
        "        sampler = ClassBalancedSamplerOld(num_per_class,task.num_classes, task.train_num,shuffle=shuffle)\n",
        "\n",
        "    else:\n",
        "        sampler = ClassBalancedSampler(task.num_classes, task.test_num,shuffle=shuffle)\n",
        "\n",
        "    loader = DataLoader(dataset, batch_size=num_per_class*task.num_classes, sampler=sampler)\n",
        "    return loader"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "EJjt84fCUJBH",
        "colab_type": "text"
      },
      "source": [
        "#Defining networks"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "up6XOKFYrzqq",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "import torchvision.models as models\n",
        "class PreTrainedResNet(nn.Module):\n",
        "  def __init__(self):\n",
        "    super(PreTrainedResNet, self).__init__()\n",
        "    \n",
        "    self.resnet18 = models.resnet18(pretrained=True)\n",
        "\n",
        "    #Set gradients to false\n",
        "    \n",
        "    for param in self.resnet18.parameters():\n",
        "        param.requires_grad = False\n",
        "    \n",
        "    num_feats = self.resnet18.fc.in_features\n",
        "    self.resnet18 = nn.Sequential(*(list(self.resnet18.children())[:-1]))\n",
        "\n",
        "    \n",
        "  def forward(self, x):\n",
        "    x = self.resnet18 (x)\n",
        "\n",
        "    x = x.view(x.shape[0],32,4,4)\n",
        "    return x"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XTQ5kfkvDHtP",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "class RelationNetwork(nn.Module):\n",
        "    \"\"\"docstring for RelationNetwork\"\"\"\n",
        "    def __init__(self,input_size,hidden_size):\n",
        "        super(RelationNetwork, self).__init__()\n",
        "        self.layer1 = nn.Sequential(\n",
        "                        nn.Conv2d(64,32,kernel_size=3,padding=1),  #padding\n",
        "                        nn.BatchNorm2d(32, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.layer2 = nn.Sequential(\n",
        "                        nn.Conv2d(32,32,kernel_size=3,padding=1),  #padding\n",
        "                        nn.BatchNorm2d(32, momentum=1, affine=True),\n",
        "                        nn.ReLU(),\n",
        "                        nn.MaxPool2d(2))\n",
        "        self.fc1 = nn.Linear(input_size,hidden_size)  #3*3: size e ax bade 2 layers\n",
        "        self.fc2 = nn.Linear(hidden_size,1)\n",
        "\n",
        "    def forward(self,x):\n",
        "        out = self.layer1(x)\n",
        "        out = self.layer2(out)\n",
        "        out = out.view(out.size(0),-1)\n",
        "        out = F.relu(self.fc1(out))\n",
        "        out = torch.sigmoid(self.fc2(out))\n",
        "        #out = self.fc2(out)\n",
        "        return out"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "XyQNkQBasrg2",
        "colab_type": "code",
        "outputId": "4ecded31-d17e-41f3-a8ed-300aa7c20f5c",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 375,
          "referenced_widgets": [
            "4cc592b3c15e4cb484765df2cf128ae6",
            "051c30c901684682a672c8fe19b94bd9",
            "f8d42ae62ea646de82236db6e41a3543",
            "8065d0adbb684d8b8fe48a9c0aefb473",
            "0c53a06680e84674a7364c0d625a5bae",
            "f80268db6fea41bd99c7bd838fbb408f",
            "b8ed4a8a00544ddeb01aabe585b93a93",
            "9d96b8059ad94cc1b66fa677144af952"
          ]
        }
      },
      "source": [
        "\n",
        "feature_encoder = PreTrainedResNet()\n",
        "#feature_encoder.apply(weights_init)\n",
        "feature_encoder.cuda(0)\n",
        "\n",
        "relation_network = RelationNetwork(FEATURE_DIM,RELATION_DIM)\n",
        "relation_network.apply(weights_init)\n",
        "relation_network.cuda(0)\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Downloading: \"https://download.pytorch.org/models/resnet18-5c106cde.pth\" to /root/.cache/torch/checkpoints/resnet18-5c106cde.pth\n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "display_data",
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "4cc592b3c15e4cb484765df2cf128ae6",
              "version_minor": 0,
              "version_major": 2
            },
            "text/plain": [
              "HBox(children=(IntProgress(value=0, max=46827520), HTML(value='')))"
            ]
          },
          "metadata": {
            "tags": []
          }
        },
        {
          "output_type": "stream",
          "text": [
            "\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RelationNetwork(\n",
              "  (layer1): Sequential(\n",
              "    (0): Conv2d(64, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (layer2): Sequential(\n",
              "    (0): Conv2d(32, 32, kernel_size=(3, 3), stride=(1, 1), padding=(1, 1))\n",
              "    (1): BatchNorm2d(32, eps=1e-05, momentum=1, affine=True, track_running_stats=True)\n",
              "    (2): ReLU()\n",
              "    (3): MaxPool2d(kernel_size=2, stride=2, padding=0, dilation=1, ceil_mode=False)\n",
              "  )\n",
              "  (fc1): Linear(in_features=32, out_features=8, bias=True)\n",
              "  (fc2): Linear(in_features=8, out_features=1, bias=True)\n",
              ")"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 15
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "spN4Mc14UNku",
        "colab_type": "text"
      },
      "source": [
        "#Optimizers"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "b1VMDHI6yROV",
        "colab_type": "code",
        "colab": {}
      },
      "source": [
        "\n",
        "feature_encoder_optim = torch.optim.Adam(feature_encoder.parameters(),lr=0.001)\n",
        "feature_encoder_scheduler = StepLR(feature_encoder_optim,step_size=100000,gamma=0.5)\n",
        "relation_network_optim = torch.optim.Adam(relation_network.parameters(),lr=0.001)\n",
        "relation_network_scheduler = StepLR(relation_network_optim,step_size=100000,gamma=0.5)"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SCRbLOEKUP3j",
        "colab_type": "text"
      },
      "source": [
        "#5way-5shot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "EazfPJogFFkh",
        "colab_type": "code",
        "outputId": "529df7e2-5fd8-48a1-9c90-dbf95bb92175",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Step 3: build graph\n",
        "print(\"Training...\")\n",
        "CLASS_NUM = 5\n",
        "x = metatest_folders\n",
        "last_accuracy = 0.0\n",
        "\n",
        "for episode in range(1000):\n",
        "\n",
        "\n",
        "    #print(episode)\n",
        "    # init dataset\n",
        "    # sample_dataloader is to obtain previous samples for compare\n",
        "    # batch_dataloader is to batch samples for training\n",
        "    task = MiniImagenetTask(x,CLASS_NUM,SAMPLE_NUM_PER_CLASS,BATCH_NUM_PER_CLASS)\n",
        "    sample_dataloader = get_mini_imagenet_data_loader(task,num_per_class=SAMPLE_NUM_PER_CLASS,split=\"train\",shuffle=False)\n",
        "    batch_dataloader = get_mini_imagenet_data_loader(task,num_per_class=BATCH_NUM_PER_CLASS,split=\"test\",shuffle=True)\n",
        "\n",
        "    # sample datas\n",
        "    samples,sample_labels = sample_dataloader.__iter__().next() \n",
        "    batches,batch_labels = batch_dataloader.__iter__().next()\n",
        "\n",
        "\n",
        "\n",
        "    # calculate features\n",
        "    sample_features = feature_encoder(Variable(samples).cuda(GPU)) \n",
        "    sample_features = sample_features.view(CLASS_NUM,SAMPLE_NUM_PER_CLASS,FEATURE_DIM,4,4)\n",
        "    sample_features = torch.sum(sample_features,1).squeeze(1)\n",
        "    batch_features = feature_encoder(Variable(batches).cuda(GPU)) \n",
        "    \n",
        "    # calculate relations\n",
        "    # each batch sample link to every samples to calculate relations\n",
        "    # to form a 100x128 matrix for relation network\n",
        "    sample_features_ext = sample_features.unsqueeze(0).repeat(BATCH_NUM_PER_CLASS*CLASS_NUM,1,1,1,1)\n",
        "    batch_features_ext = batch_features.unsqueeze(0).repeat(CLASS_NUM,1,1,1,1)\n",
        "    batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
        "    relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2).view(-1,FEATURE_DIM*2,4,4)\n",
        "    relations = relation_network(relation_pairs).view(-1,CLASS_NUM)\n",
        "\n",
        "    mse = nn.MSELoss().cuda(GPU)\n",
        "    crossen = nn.CrossEntropyLoss().cuda(0)\n",
        "    one_hot_labels = Variable(torch.zeros(BATCH_NUM_PER_CLASS*CLASS_NUM, CLASS_NUM).scatter_(1, batch_labels.view(-1,1), 1).cuda(GPU))\n",
        "    loss = mse(relations,one_hot_labels)\n",
        "    #print(\"loss driven\")\n",
        "\n",
        "    # training\n",
        "\n",
        "    feature_encoder.zero_grad()\n",
        "    relation_network.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(feature_encoder.parameters(),0.5)\n",
        "    torch.nn.utils.clip_grad_norm_(relation_network.parameters(),0.5)\n",
        "\n",
        "    feature_encoder_optim.step()\n",
        "    relation_network_optim.step()\n",
        "\n",
        "    feature_encoder_scheduler.step(episode)\n",
        "    relation_network_scheduler.step(episode)\n",
        "\n",
        "\n",
        "    if (episode+1)%100 == 0:\n",
        "      print(\"episode:\",episode+1,\"loss\",loss.item())\n",
        "\n",
        "    if (episode+1)%1000 == 0:\n",
        "\n",
        "        # test\n",
        "        print(\"Testing...\")\n",
        "        accuracies = []\n",
        "        for i in range(100):\n",
        "            total_rewards = 0\n",
        "            #counter = 0       #remove\n",
        "            task = MiniImagenetTask(metaval_folders,CLASS_NUM,SAMPLE_NUM_PER_CLASS,BATCH_NUM_PER_CLASS)\n",
        "            sample_dataloader = get_mini_imagenet_data_loader(task,num_per_class=SAMPLE_NUM_PER_CLASS,split=\"train\",shuffle=False)\n",
        "            num_per_class = 5   #3\n",
        "            test_dataloader = get_mini_imagenet_data_loader(task,num_per_class=num_per_class,split=\"test\",shuffle=False)\n",
        "\n",
        "            sample_images,sample_labels = sample_dataloader.__iter__().next()\n",
        "            for test_images,test_labels in test_dataloader:\n",
        "                batch_size = test_labels.shape[0]\n",
        "                # calculate features\n",
        "                sample_features = feature_encoder(Variable(sample_images).cuda(GPU)) # 5x64\n",
        "                sample_features = sample_features.view(CLASS_NUM,SAMPLE_NUM_PER_CLASS,FEATURE_DIM,4,4)\n",
        "                sample_features = torch.sum(sample_features,1).squeeze(1)\n",
        "                test_features = feature_encoder(Variable(test_images).cuda(GPU)) # 20x64\n",
        "\n",
        "                # calculate relations\n",
        "                # each batch sample link to every samples to calculate relations\n",
        "                # to form a 100x128 matrix for relation network\n",
        "                sample_features_ext = sample_features.unsqueeze(0).repeat(batch_size,1,1,1,1)\n",
        "\n",
        "                test_features_ext = test_features.unsqueeze(0).repeat(1*CLASS_NUM,1,1,1,1)\n",
        "                test_features_ext = torch.transpose(test_features_ext,0,1)\n",
        "                relation_pairs = torch.cat((sample_features_ext,test_features_ext),2).view(-1,FEATURE_DIM*2,4,4)\n",
        "                relations = relation_network(relation_pairs).view(-1,CLASS_NUM)\n",
        "\n",
        "                _,predict_labels = torch.max(relations.data,1)\n",
        "\n",
        "                rewards = [1 if predict_labels[j]==test_labels[j] else 0 for j in range(batch_size)]\n",
        "\n",
        "                total_rewards += np.sum(rewards)\n",
        "                #counter +=batch_size   #remove\n",
        "\n",
        "\n",
        "            accuracy = total_rewards/1.0/CLASS_NUM/BATCH_NUM_PER_CLASS\n",
        "            #accuracy = total_rewards/1.0/counter\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "        test_accuracy,h = mean_confidence_interval(accuracies)\n",
        "\n",
        "        print(\"test accuracy:\",test_accuracy,\"h:\",h)\n",
        "\n",
        "        if test_accuracy > last_accuracy:\n",
        "\n",
        "            # save networks\n",
        "            torch.save(feature_encoder.state_dict(),str(\"miniimagenet/models/new/miniimagenet_feature_encoder2_\" + str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"))\n",
        "            torch.save(relation_network.state_dict(),str(\"miniimagenet/models/new/miniimagenet_relation_network2_\"+ str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"))\n",
        "\n",
        "            print(\"save networks for episode:\",episode)\n",
        "\n",
        "            last_accuracy = test_accuracy\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "episode: 100 loss 0.02784169837832451\n",
            "episode: 200 loss 0.030300322920084\n",
            "episode: 300 loss 0.046183548867702484\n",
            "episode: 400 loss 0.03398463502526283\n",
            "episode: 500 loss 0.04983297362923622\n",
            "episode: 600 loss 0.018380021676421165\n",
            "episode: 700 loss 0.01635332591831684\n",
            "episode: 800 loss 0.04367595165967941\n",
            "episode: 900 loss 0.026291634887456894\n",
            "episode: 1000 loss 0.04830363392829895\n",
            "Testing...\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "error",
          "ename": "KeyboardInterrupt",
          "evalue": "ignored",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-166-13f64381bd1a>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     75\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     76\u001b[0m             \u001b[0msample_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0msample_labels\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0msample_dataloader\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m__iter__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mnext\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 77\u001b[0;31m             \u001b[0;32mfor\u001b[0m \u001b[0mtest_images\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0mtest_labels\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mtest_dataloader\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     78\u001b[0m                 \u001b[0mbatch_size\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtest_labels\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     79\u001b[0m                 \u001b[0;31m# calculate features\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m__next__\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    343\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    344\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__next__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 345\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    346\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_num_yielded\u001b[0m \u001b[0;34m+=\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    347\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_kind\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0m_DatasetKind\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mIterable\u001b[0m \u001b[0;32mand\u001b[0m\u001b[0;31m \u001b[0m\u001b[0;31m\\\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/dataloader.py\u001b[0m in \u001b[0;36m_next_data\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    383\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m_next_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    384\u001b[0m         \u001b[0mindex\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_next_index\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 385\u001b[0;31m         \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_dataset_fetcher\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mindex\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# may raise StopIteration\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    386\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_pin_memory\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    387\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_utils\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpin_memory\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mdata\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36mfetch\u001b[0;34m(self, possibly_batched_index)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/torch/utils/data/_utils/fetch.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m     42\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0mfetch\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     43\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mauto_collation\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 44\u001b[0;31m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0midx\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     45\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     46\u001b[0m             \u001b[0mdata\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mdataset\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mpossibly_batched_index\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m<ipython-input-27-b76da949b2f6>\u001b[0m in \u001b[0;36m__getitem__\u001b[0;34m(self, idx)\u001b[0m\n\u001b[1;32m     60\u001b[0m     \u001b[0;32mdef\u001b[0m \u001b[0m__getitem__\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0midx\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     61\u001b[0m         \u001b[0mimage_root\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mimage_roots\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0midx\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 62\u001b[0;31m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mImage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mopen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mimage_root\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     63\u001b[0m         \u001b[0mimage\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mimage\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mconvert\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m'RGB'\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     64\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtransform\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.6/dist-packages/PIL/Image.py\u001b[0m in \u001b[0;36mopen\u001b[0;34m(fp, mode)\u001b[0m\n\u001b[1;32m   2816\u001b[0m         \u001b[0mexclusive_fp\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;32mTrue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2817\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 2818\u001b[0;31m     \u001b[0mprefix\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m16\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   2819\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   2820\u001b[0m     \u001b[0mpreinit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pcAtRfqjDpfw",
        "colab_type": "code",
        "outputId": "3920ce59-2f2c-4b04-ee44-f2e99af4a866",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "        print(\"Testing...\")\n",
        "        accuracies = []\n",
        "        for i in range(20):\n",
        "            print(i)\n",
        "            total_rewards = 0\n",
        "            #counter = 0       #remove\n",
        "            task = MiniImagenetTask(metaval_folders,CLASS_NUM,SAMPLE_NUM_PER_CLASS,BATCH_NUM_PER_CLASS)\n",
        "            sample_dataloader = get_mini_imagenet_data_loader(task,num_per_class=SAMPLE_NUM_PER_CLASS,split=\"train\",shuffle=False)\n",
        "            num_per_class = 5   #3\n",
        "            test_dataloader = get_mini_imagenet_data_loader(task,num_per_class=num_per_class,split=\"test\",shuffle=False)\n",
        "\n",
        "            sample_images,sample_labels = sample_dataloader.__iter__().next()\n",
        "            for test_images,test_labels in test_dataloader:\n",
        "                batch_size = test_labels.shape[0]\n",
        "                # calculate features\n",
        "                sample_features = feature_encoder(Variable(sample_images).cuda(GPU)) # 5x64\n",
        "                sample_features = sample_features.view(CLASS_NUM,SAMPLE_NUM_PER_CLASS,FEATURE_DIM,4,4)\n",
        "                sample_features = torch.sum(sample_features,1).squeeze(1)\n",
        "                test_features = feature_encoder(Variable(test_images).cuda(GPU)) # 20x64\n",
        "\n",
        "                # calculate relations\n",
        "                # each batch sample link to every samples to calculate relations\n",
        "                # to form a 100x128 matrix for relation network\n",
        "                sample_features_ext = sample_features.unsqueeze(0).repeat(batch_size,1,1,1,1)\n",
        "\n",
        "                test_features_ext = test_features.unsqueeze(0).repeat(1*CLASS_NUM,1,1,1,1)\n",
        "                test_features_ext = torch.transpose(test_features_ext,0,1)\n",
        "                relation_pairs = torch.cat((sample_features_ext,test_features_ext),2).view(-1,FEATURE_DIM*2,4,4)\n",
        "                relations = relation_network(relation_pairs).view(-1,CLASS_NUM)\n",
        "\n",
        "                _,predict_labels = torch.max(relations.data,1)\n",
        "\n",
        "                rewards = [1 if predict_labels[j]==test_labels[j] else 0 for j in range(batch_size)]\n",
        "\n",
        "                total_rewards += np.sum(rewards)\n",
        "                #counter +=batch_size   #remove\n",
        "\n",
        "\n",
        "            accuracy = total_rewards/1.0/CLASS_NUM/BATCH_NUM_PER_CLASS\n",
        "            #accuracy = total_rewards/1.0/counter\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "        test_accuracy,h = mean_confidence_interval(accuracies)\n",
        "\n",
        "        print(\"test accuracy:\",test_accuracy,\"h:\",h)\n",
        "\n",
        "        if test_accuracy > last_accuracy:\n",
        "\n",
        "            # save networks\n",
        "            #torch.save(feature_encoder.state_dict(),str(\"miniimagenet/models/new/miniimagenet_feature_encoder2_\" + str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"))\n",
        "            #torch.save(relation_network.state_dict(),str(\"miniimagenet/models/new/miniimagenet_relation_network2_\"+ str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"))\n",
        "\n",
        "            print(\"save networks for episode:\",episode)\n",
        "\n",
        "            last_accuracy = test_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "test accuracy: 0.733 h: 0.03735175356152608\n",
            "save networks for episode: 999\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vcZIo0DvVrlY",
        "colab_type": "text"
      },
      "source": [
        "#5way-1shot"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "id": "pVuRPnDCmJir",
        "colab": {}
      },
      "source": [
        "\n",
        "feature_encoder = PreTrainedResNet()\n",
        "#feature_encoder.apply(weights_init)\n",
        "feature_encoder.cuda(0)\n",
        "\n",
        "relation_network = RelationNetwork(FEATURE_DIM,RELATION_DIM)\n",
        "relation_network.apply(weights_init)\n",
        "relation_network.cuda(0)\n"
      ],
      "execution_count": 0,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "3085c894-2066-4d94-d2b9-93c66debb173",
        "id": "PywhOCLCU4Qo",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        }
      },
      "source": [
        "# Step 3: build graph\n",
        "print(\"Training...\")\n",
        "# Hyper Parameters\n",
        "CLASS_NUM = 5\n",
        "SAMPLE_NUM_PER_CLASS = 1   #5\n",
        "BATCH_NUM_PER_CLASS = 10   #10\n",
        "EPISODE = 1000\n",
        "TEST_EPISODE = 20\n",
        "\n",
        "\n",
        "x = metatest_folders\n",
        "last_accuracy = 0.0\n",
        "\n",
        "for episode in range(1000):\n",
        "\n",
        "\n",
        "    #print(episode)\n",
        "    # init dataset\n",
        "    # sample_dataloader is to obtain previous samples for compare\n",
        "    # batch_dataloader is to batch samples for training\n",
        "    task = MiniImagenetTask(x,CLASS_NUM,SAMPLE_NUM_PER_CLASS,BATCH_NUM_PER_CLASS)\n",
        "    sample_dataloader = get_mini_imagenet_data_loader(task,num_per_class=SAMPLE_NUM_PER_CLASS,split=\"train\",shuffle=False)\n",
        "    batch_dataloader = get_mini_imagenet_data_loader(task,num_per_class=BATCH_NUM_PER_CLASS,split=\"test\",shuffle=True)\n",
        "\n",
        "    # sample datas\n",
        "    samples,sample_labels = sample_dataloader.__iter__().next() \n",
        "    batches,batch_labels = batch_dataloader.__iter__().next()\n",
        "\n",
        "\n",
        "\n",
        "    # calculate features\n",
        "    sample_features = feature_encoder(Variable(samples).cuda(GPU)) \n",
        "    sample_features = sample_features.view(CLASS_NUM,SAMPLE_NUM_PER_CLASS,FEATURE_DIM,4,4)\n",
        "    sample_features = torch.sum(sample_features,1).squeeze(1)\n",
        "    batch_features = feature_encoder(Variable(batches).cuda(GPU)) \n",
        "    \n",
        "    # calculate relations\n",
        "    # each batch sample link to every samples to calculate relations\n",
        "    # to form a 100x128 matrix for relation network\n",
        "    sample_features_ext = sample_features.unsqueeze(0).repeat(BATCH_NUM_PER_CLASS*CLASS_NUM,1,1,1,1)\n",
        "    batch_features_ext = batch_features.unsqueeze(0).repeat(CLASS_NUM,1,1,1,1)\n",
        "    batch_features_ext = torch.transpose(batch_features_ext,0,1)\n",
        "    relation_pairs = torch.cat((sample_features_ext,batch_features_ext),2).view(-1,FEATURE_DIM*2,4,4)\n",
        "    relations = relation_network(relation_pairs).view(-1,CLASS_NUM)\n",
        "\n",
        "    mse = nn.MSELoss().cuda(GPU)\n",
        "    crossen = nn.CrossEntropyLoss().cuda(0)\n",
        "    one_hot_labels = Variable(torch.zeros(BATCH_NUM_PER_CLASS*CLASS_NUM, CLASS_NUM).scatter_(1, batch_labels.view(-1,1), 1).cuda(GPU))\n",
        "    loss = mse(relations,one_hot_labels)\n",
        "    #print(\"loss driven\")\n",
        "\n",
        "    # training\n",
        "\n",
        "    feature_encoder.zero_grad()\n",
        "    relation_network.zero_grad()\n",
        "    loss.backward()\n",
        "\n",
        "    torch.nn.utils.clip_grad_norm_(feature_encoder.parameters(),0.5)\n",
        "    torch.nn.utils.clip_grad_norm_(relation_network.parameters(),0.5)\n",
        "\n",
        "    feature_encoder_optim.step()\n",
        "    relation_network_optim.step()\n",
        "\n",
        "    feature_encoder_scheduler.step(episode)\n",
        "    relation_network_scheduler.step(episode)\n",
        "\n",
        "\n",
        "    if (episode+1)%10 == 0:\n",
        "      print(\"episode:\",episode+1,\"loss\",loss.item())\n"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Training...\n",
            "episode: 10 loss 0.32025179266929626\n",
            "episode: 20 loss 0.2504648268222809\n",
            "episode: 30 loss 0.18935303390026093\n",
            "episode: 40 loss 0.1612299680709839\n",
            "episode: 50 loss 0.16015499830245972\n",
            "episode: 60 loss 0.1596599817276001\n",
            "episode: 70 loss 0.15839271247386932\n",
            "episode: 80 loss 0.151975616812706\n",
            "episode: 90 loss 0.15307706594467163\n",
            "episode: 100 loss 0.1280609369277954\n",
            "episode: 110 loss 0.12190590798854828\n",
            "episode: 120 loss 0.13942624628543854\n",
            "episode: 130 loss 0.10935577750205994\n",
            "episode: 140 loss 0.1136331856250763\n",
            "episode: 150 loss 0.12622761726379395\n",
            "episode: 160 loss 0.10424743592739105\n",
            "episode: 170 loss 0.1000719666481018\n",
            "episode: 180 loss 0.10642261058092117\n",
            "episode: 190 loss 0.11744077503681183\n",
            "episode: 200 loss 0.10847144573926926\n",
            "episode: 210 loss 0.09370134770870209\n",
            "episode: 220 loss 0.13775424659252167\n",
            "episode: 230 loss 0.13025178015232086\n",
            "episode: 240 loss 0.08624213188886642\n",
            "episode: 250 loss 0.09775585681200027\n",
            "episode: 260 loss 0.09387928992509842\n",
            "episode: 270 loss 0.1292164921760559\n",
            "episode: 280 loss 0.09610122442245483\n",
            "episode: 290 loss 0.10133709758520126\n",
            "episode: 300 loss 0.11090553551912308\n",
            "episode: 310 loss 0.1349465250968933\n",
            "episode: 320 loss 0.08962862938642502\n",
            "episode: 330 loss 0.07660273462533951\n",
            "episode: 340 loss 0.09517967700958252\n",
            "episode: 350 loss 0.10911005735397339\n",
            "episode: 360 loss 0.09310272336006165\n",
            "episode: 370 loss 0.09279059618711472\n",
            "episode: 380 loss 0.09904666244983673\n",
            "episode: 390 loss 0.1110934466123581\n",
            "episode: 400 loss 0.08008963614702225\n",
            "episode: 410 loss 0.0699765756726265\n",
            "episode: 420 loss 0.10766895860433578\n",
            "episode: 430 loss 0.10283143818378448\n",
            "episode: 440 loss 0.06510380655527115\n",
            "episode: 450 loss 0.1175430566072464\n",
            "episode: 460 loss 0.1200687438249588\n",
            "episode: 470 loss 0.11763119697570801\n",
            "episode: 480 loss 0.09941931068897247\n",
            "episode: 490 loss 0.08469492942094803\n",
            "episode: 500 loss 0.09121004492044449\n",
            "episode: 510 loss 0.11207355558872223\n",
            "episode: 520 loss 0.08950536698102951\n",
            "episode: 530 loss 0.10481694340705872\n",
            "episode: 540 loss 0.09027896821498871\n",
            "episode: 550 loss 0.09566274285316467\n",
            "episode: 560 loss 0.048694755882024765\n",
            "episode: 570 loss 0.08114583790302277\n",
            "episode: 580 loss 0.08288988471031189\n",
            "episode: 590 loss 0.08222583681344986\n",
            "episode: 600 loss 0.06017217040061951\n",
            "episode: 610 loss 0.09430555254220963\n",
            "episode: 620 loss 0.11033938825130463\n",
            "episode: 630 loss 0.06492259353399277\n",
            "episode: 640 loss 0.09160321205854416\n",
            "episode: 650 loss 0.07182256132364273\n",
            "episode: 660 loss 0.10006796568632126\n",
            "episode: 670 loss 0.06259763985872269\n",
            "episode: 680 loss 0.08224236220121384\n",
            "episode: 690 loss 0.0906776562333107\n",
            "episode: 700 loss 0.06648444384336472\n",
            "episode: 710 loss 0.08667685091495514\n",
            "episode: 720 loss 0.08836113661527634\n",
            "episode: 730 loss 0.08824047446250916\n",
            "episode: 740 loss 0.05977572873234749\n",
            "episode: 750 loss 0.0799274668097496\n",
            "episode: 760 loss 0.06460070610046387\n",
            "episode: 770 loss 0.13499034941196442\n",
            "episode: 780 loss 0.09327080100774765\n",
            "episode: 790 loss 0.11239281296730042\n",
            "episode: 800 loss 0.11927706003189087\n",
            "episode: 810 loss 0.10177893191576004\n",
            "episode: 820 loss 0.06883201748132706\n",
            "episode: 830 loss 0.07735174894332886\n",
            "episode: 840 loss 0.07054606825113297\n",
            "episode: 850 loss 0.06839600205421448\n",
            "episode: 860 loss 0.06506038457155228\n",
            "episode: 870 loss 0.061359040439128876\n",
            "episode: 880 loss 0.0649137869477272\n",
            "episode: 890 loss 0.07734525948762894\n",
            "episode: 900 loss 0.06974311172962189\n",
            "episode: 910 loss 0.08024796843528748\n",
            "episode: 920 loss 0.10324680060148239\n",
            "episode: 930 loss 0.07449524104595184\n",
            "episode: 940 loss 0.06165515258908272\n",
            "episode: 950 loss 0.1044858768582344\n",
            "episode: 960 loss 0.07887272536754608\n",
            "episode: 970 loss 0.06747300177812576\n",
            "episode: 980 loss 0.07844158262014389\n",
            "episode: 990 loss 0.047641728073358536\n",
            "episode: 1000 loss 0.059684161096811295\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "colab_type": "code",
        "outputId": "a4fcc9d7-c1b0-4d8e-e033-2e964c03af5c",
        "id": "y5LiWTfnU4Qs",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 408
        }
      },
      "source": [
        "        print(\"Testing...\")\n",
        "        accuracies = []\n",
        "        for i in range(20):\n",
        "            print(i)\n",
        "            total_rewards = 0\n",
        "            #counter = 0       #remove\n",
        "            task = MiniImagenetTask(metaval_folders,CLASS_NUM,SAMPLE_NUM_PER_CLASS,BATCH_NUM_PER_CLASS)\n",
        "            sample_dataloader = get_mini_imagenet_data_loader(task,num_per_class=SAMPLE_NUM_PER_CLASS,split=\"train\",shuffle=False)\n",
        "            num_per_class = 5   #3\n",
        "            test_dataloader = get_mini_imagenet_data_loader(task,num_per_class=num_per_class,split=\"test\",shuffle=False)\n",
        "\n",
        "            sample_images,sample_labels = sample_dataloader.__iter__().next()\n",
        "            for test_images,test_labels in test_dataloader:\n",
        "                batch_size = test_labels.shape[0]\n",
        "                # calculate features\n",
        "                sample_features = feature_encoder(Variable(sample_images).cuda(GPU)) # 5x64\n",
        "                sample_features = sample_features.view(CLASS_NUM,SAMPLE_NUM_PER_CLASS,FEATURE_DIM,4,4)\n",
        "                sample_features = torch.sum(sample_features,1).squeeze(1)\n",
        "                test_features = feature_encoder(Variable(test_images).cuda(GPU)) # 20x64\n",
        "\n",
        "                # calculate relations\n",
        "                # each batch sample link to every samples to calculate relations\n",
        "                # to form a 100x128 matrix for relation network\n",
        "                sample_features_ext = sample_features.unsqueeze(0).repeat(batch_size,1,1,1,1)\n",
        "\n",
        "                test_features_ext = test_features.unsqueeze(0).repeat(1*CLASS_NUM,1,1,1,1)\n",
        "                test_features_ext = torch.transpose(test_features_ext,0,1)\n",
        "                relation_pairs = torch.cat((sample_features_ext,test_features_ext),2).view(-1,FEATURE_DIM*2,4,4)\n",
        "                relations = relation_network(relation_pairs).view(-1,CLASS_NUM)\n",
        "\n",
        "                _,predict_labels = torch.max(relations.data,1)\n",
        "\n",
        "                rewards = [1 if predict_labels[j]==test_labels[j] else 0 for j in range(batch_size)]\n",
        "\n",
        "                total_rewards += np.sum(rewards)\n",
        "                #counter +=batch_size   #remove\n",
        "\n",
        "\n",
        "            accuracy = total_rewards/1.0/CLASS_NUM/BATCH_NUM_PER_CLASS\n",
        "            #accuracy = total_rewards/1.0/counter\n",
        "            accuracies.append(accuracy)\n",
        "\n",
        "\n",
        "        test_accuracy,h = mean_confidence_interval(accuracies)\n",
        "\n",
        "        print(\"test accuracy:\",test_accuracy,\"h:\",h)\n",
        "\n",
        "        if test_accuracy > last_accuracy:\n",
        "\n",
        "            # save networks\n",
        "            torch.save(feature_encoder.state_dict(),str(\"miniimagenet/models/new/miniimagenet_feature_encoder2_\" + str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"))\n",
        "            torch.save(relation_network.state_dict(),str(\"miniimagenet/models/new/miniimagenet_relation_network2_\"+ str(CLASS_NUM) +\"way_\" + str(SAMPLE_NUM_PER_CLASS) +\"shot.pkl\"))\n",
        "\n",
        "            print(\"save networks for episode:\",episode)\n",
        "\n",
        "            last_accuracy = test_accuracy"
      ],
      "execution_count": 0,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Testing...\n",
            "0\n",
            "1\n",
            "2\n",
            "3\n",
            "4\n",
            "5\n",
            "6\n",
            "7\n",
            "8\n",
            "9\n",
            "10\n",
            "11\n",
            "12\n",
            "13\n",
            "14\n",
            "15\n",
            "16\n",
            "17\n",
            "18\n",
            "19\n",
            "test accuracy: 0.5810000000000001 h: 0.06705074207791162\n",
            "save networks for episode: 999\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}